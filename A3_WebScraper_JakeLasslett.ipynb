{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "import datetime as dt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementClickInterceptedException\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is WebScraper for Reddit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "## This is pulling my credentials from a secret config.py file which will not be shared\n",
    "from config import client_id, client_secret, user_agent, password, username\n",
    "\n",
    "# Set up PRAW with your credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    password=password,\n",
    "    user_agent=user_agent,\n",
    "    username=username,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for 2023-12-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakel\\AppData\\Local\\Temp\\ipykernel_11312\\3566540188.py:11: DeprecationWarning: Positional arguments for 'BaseListingMixin.top' will no longer be supported in PRAW 8.\n",
      "Call this function with 'time_filter' as a keyword argument.\n",
      "  for post in subreddit.top('day', limit=30):  # Fetches top 30 posts of the day\n"
     ]
    },
    {
     "ename": "NotFound",
     "evalue": "received 404 HTTP response",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotFound\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[137], line 78\u001B[0m\n\u001B[0;32m     74\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mData scraped and saved to reddit_data_combined.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     76\u001B[0m \u001B[38;5;66;03m# Run the data collection function directly for now\u001B[39;00m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;66;03m# In production, you might want to use a scheduler as before\u001B[39;00m\n\u001B[1;32m---> 78\u001B[0m \u001B[43mrun_data_collection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[137], line 66\u001B[0m, in \u001B[0;36mrun_data_collection\u001B[1;34m()\u001B[0m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_data_collection\u001B[39m():\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mScraping data for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdt\u001B[38;5;241m.\u001B[39mdatetime\u001B[38;5;241m.\u001B[39mnow()\u001B[38;5;241m.\u001B[39mstrftime(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mY-\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mm-\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 66\u001B[0m     daily_data \u001B[38;5;241m=\u001B[39m \u001B[43mscrape_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     67\u001B[0m     df_daily \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(daily_data)\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# Combine with existing data and remove duplicates\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[137], line 24\u001B[0m, in \u001B[0;36mscrape_data\u001B[1;34m()\u001B[0m\n\u001B[0;32m     22\u001B[0m new_data \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m subreddit_name \u001B[38;5;129;01min\u001B[39;00m subreddits:\n\u001B[1;32m---> 24\u001B[0m     top_posts \u001B[38;5;241m=\u001B[39m \u001B[43mget_top_posts\u001B[49m\u001B[43m(\u001B[49m\u001B[43msubreddit_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m post \u001B[38;5;129;01min\u001B[39;00m top_posts:\n\u001B[0;32m     26\u001B[0m         post_data \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     27\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtitle\u001B[39m\u001B[38;5;124m'\u001B[39m: post\u001B[38;5;241m.\u001B[39mtitle,\n\u001B[0;32m     28\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m: post\u001B[38;5;241m.\u001B[39mselftext,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     36\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msubreddit\u001B[39m\u001B[38;5;124m'\u001B[39m: subreddit_name\n\u001B[0;32m     37\u001B[0m         }\n",
      "Cell \u001B[1;32mIn[137], line 11\u001B[0m, in \u001B[0;36mget_top_posts\u001B[1;34m(subreddit_name)\u001B[0m\n\u001B[0;32m      9\u001B[0m subreddit \u001B[38;5;241m=\u001B[39m reddit\u001B[38;5;241m.\u001B[39msubreddit(subreddit_name)\n\u001B[0;32m     10\u001B[0m posts \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m---> 11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m post \u001B[38;5;129;01min\u001B[39;00m subreddit\u001B[38;5;241m.\u001B[39mtop(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mday\u001B[39m\u001B[38;5;124m'\u001B[39m, limit\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m30\u001B[39m):  \u001B[38;5;66;03m# Fetches top 30 posts of the day\u001B[39;00m\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_relevant(post\u001B[38;5;241m.\u001B[39mtitle) \u001B[38;5;129;01mor\u001B[39;00m is_relevant(post\u001B[38;5;241m.\u001B[39mselftext): \u001B[38;5;66;03m## Searches for the relevent keywords in the title and text\u001B[39;00m\n\u001B[0;32m     13\u001B[0m         posts\u001B[38;5;241m.\u001B[39mappend(post)\n",
      "File \u001B[1;32m~\\Documents\\Uni\\MA5851 Data Science Master Class 1\\Assignments\\A3_Capstone\\A3_JakeLasslett_2\\lib\\site-packages\\praw\\models\\listing\\generator.py:63\u001B[0m, in \u001B[0;36mListingGenerator.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m()\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_listing \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_list_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_listing):\n\u001B[1;32m---> 63\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_list_index \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39myielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\Documents\\Uni\\MA5851 Data Science Master Class 1\\Assignments\\A3_Capstone\\A3_JakeLasslett_2\\lib\\site-packages\\praw\\models\\listing\\generator.py:89\u001B[0m, in \u001B[0;36mListingGenerator._next_batch\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exhausted:\n\u001B[0;32m     87\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m()\n\u001B[1;32m---> 89\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_listing \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reddit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     90\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_listing \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_extract_sublist(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_listing)\n\u001B[0;32m     91\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_list_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[1;32m~\\Documents\\Uni\\MA5851 Data Science Master Class 1\\Assignments\\A3_Capstone\\A3_JakeLasslett_2\\lib\\site-packages\\praw\\util\\deprecate_args.py:43\u001B[0m, in \u001B[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     36\u001B[0m     arg_string \u001B[38;5;241m=\u001B[39m _generate_arg_string(_old_args[: \u001B[38;5;28mlen\u001B[39m(args)])\n\u001B[0;32m     37\u001B[0m     warn(\n\u001B[0;32m     38\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPositional arguments for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m will no longer be\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     39\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m supported in PRAW 8.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mCall this function with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00marg_string\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     40\u001B[0m         \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m,\n\u001B[0;32m     41\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[0;32m     42\u001B[0m     )\n\u001B[1;32m---> 43\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(_old_args, args)), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Documents\\Uni\\MA5851 Data Science Master Class 1\\Assignments\\A3_Capstone\\A3_JakeLasslett_2\\lib\\site-packages\\praw\\reddit.py:712\u001B[0m, in \u001B[0;36mReddit.get\u001B[1;34m(self, path, params)\u001B[0m\n\u001B[0;32m    699\u001B[0m \u001B[38;5;129m@_deprecate_args\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpath\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparams\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    700\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(\n\u001B[0;32m    701\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    704\u001B[0m     params: Optional[Union[\u001B[38;5;28mstr\u001B[39m, Dict[\u001B[38;5;28mstr\u001B[39m, Union[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mint\u001B[39m]]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    705\u001B[0m ):\n\u001B[0;32m    706\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return parsed objects returned from a GET request to ``path``.\u001B[39;00m\n\u001B[0;32m    707\u001B[0m \n\u001B[0;32m    708\u001B[0m \u001B[38;5;124;03m    :param path: The path to fetch.\u001B[39;00m\n\u001B[0;32m    709\u001B[0m \u001B[38;5;124;03m    :param params: The query parameters to add to the request (default: ``None``).\u001B[39;00m\n\u001B[0;32m    710\u001B[0m \n\u001B[0;32m    711\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 712\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_objectify_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mGET\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Uni\\MA5851 Data Science Master Class 1\\Assignments\\A3_Capstone\\A3_JakeLasslett_2\\lib\\site-packages\\praw\\reddit.py:517\u001B[0m, in \u001B[0;36mReddit._objectify_request\u001B[1;34m(self, data, files, json, method, params, path)\u001B[0m\n\u001B[0;32m    491\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_objectify_request\u001B[39m(\n\u001B[0;32m    492\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    493\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    499\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    500\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m    501\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Run a request through the ``Objector``.\u001B[39;00m\n\u001B[0;32m    502\u001B[0m \n\u001B[0;32m    503\u001B[0m \u001B[38;5;124;03m    :param data: Dictionary, bytes, or file-like object to send in the body of the\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    514\u001B[0m \n\u001B[0;32m    515\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m    516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_objector\u001B[38;5;241m.\u001B[39mobjectify(\n\u001B[1;32m--> 517\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    518\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    519\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfiles\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfiles\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    520\u001B[0m \u001B[43m            \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    521\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    522\u001B[0m \u001B[43m            \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    523\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    524\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    525\u001B[0m     )\n",
      "File \u001B[1;32m~\\Documents\\Uni\\MA5851 Data Science Master Class 1\\Assignments\\A3_Capstone\\A3_JakeLasslett_2\\lib\\site-packages\\praw\\util\\deprecate_args.py:43\u001B[0m, in \u001B[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     36\u001B[0m     arg_string \u001B[38;5;241m=\u001B[39m _generate_arg_string(_old_args[: \u001B[38;5;28mlen\u001B[39m(args)])\n\u001B[0;32m     37\u001B[0m     warn(\n\u001B[0;32m     38\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPositional arguments for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m will no longer be\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     39\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m supported in PRAW 8.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mCall this function with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00marg_string\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     40\u001B[0m         \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m,\n\u001B[0;32m     41\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[0;32m     42\u001B[0m     )\n\u001B[1;32m---> 43\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(_old_args, args)), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Documents\\Uni\\MA5851 Data Science Master Class 1\\Assignments\\A3_Capstone\\A3_JakeLasslett_2\\lib\\site-packages\\praw\\reddit.py:941\u001B[0m, in \u001B[0;36mReddit.request\u001B[1;34m(self, data, files, json, method, params, path)\u001B[0m\n\u001B[0;32m    939\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ClientException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAt most one of \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is supported.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    940\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 941\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_core\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    942\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    943\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfiles\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfiles\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    944\u001B[0m \u001B[43m        \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    945\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    946\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    947\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    948\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    949\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m BadRequest \u001B[38;5;28;01mas\u001B[39;00m exception:\n\u001B[0;32m    950\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Documents\\Uni\\MA5851 Data Science Master Class 1\\Assignments\\A3_Capstone\\A3_JakeLasslett_2\\lib\\site-packages\\prawcore\\sessions.py:328\u001B[0m, in \u001B[0;36mSession.request\u001B[1;34m(self, method, path, data, files, json, params, timeout)\u001B[0m\n\u001B[0;32m    326\u001B[0m     json[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapi_type\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    327\u001B[0m url \u001B[38;5;241m=\u001B[39m urljoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_requestor\u001B[38;5;241m.\u001B[39moauth_url, path)\n\u001B[1;32m--> 328\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request_with_retries\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    329\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    330\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiles\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfiles\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    331\u001B[0m \u001B[43m    \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    332\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    333\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    334\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    335\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    336\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Uni\\MA5851 Data Science Master Class 1\\Assignments\\A3_Capstone\\A3_JakeLasslett_2\\lib\\site-packages\\prawcore\\sessions.py:267\u001B[0m, in \u001B[0;36mSession._request_with_retries\u001B[1;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001B[0m\n\u001B[0;32m    254\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_retry(\n\u001B[0;32m    255\u001B[0m         data,\n\u001B[0;32m    256\u001B[0m         files,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    264\u001B[0m         url,\n\u001B[0;32m    265\u001B[0m     )\n\u001B[0;32m    266\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mSTATUS_EXCEPTIONS:\n\u001B[1;32m--> 267\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mSTATUS_EXCEPTIONS[response\u001B[38;5;241m.\u001B[39mstatus_code](response)\n\u001B[0;32m    268\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m codes[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mno_content\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m    269\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mNotFound\u001B[0m: received 404 HTTP response"
     ]
    }
   ],
   "source": [
    "subreddits = ['N_N_N', 'CombatFootage', 'israelpalestine', 'AskMiddleEast', 'israelhamaswar']\n",
    "keywords = ['palestine', 'israel', 'gaza', 'west bank', 'hamas', 'idf', 'settlements']\n",
    "\n",
    "def is_relevant(text):\n",
    "    text_lower = text.lower()\n",
    "    return any(keyword in text_lower for keyword in keywords)\n",
    "\n",
    "def get_top_posts(subreddit_name):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts = []\n",
    "    for post in subreddit.top('day', limit=30):  # Fetches top 30 posts of the day\n",
    "        if is_relevant(post.title) or is_relevant(post.selftext): ## Searches for the relevent keywords in the title and text\n",
    "            posts.append(post)\n",
    "    return posts[:15]  # Limit to 15 relevent posts\n",
    "\n",
    "def get_comments(post):\n",
    "    post.comments.replace_more(limit=0)  # This line ensures that all MoreComments objects are removed\n",
    "    comments = post.comments.list()\n",
    "    return comments[:5]  # Limit to the first 5 comments\n",
    "\n",
    "def scrape_data():\n",
    "    new_data = []\n",
    "    for subreddit_name in subreddits:\n",
    "        top_posts = get_top_posts(subreddit_name)\n",
    "        for post in top_posts:\n",
    "            post_data = {\n",
    "                'title': post.title,\n",
    "                'text': post.selftext,\n",
    "                'upvotes': post.score,\n",
    "                'author': str(post.author),\n",
    "                'date': dt.datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'id': post.id,\n",
    "                'flair': post.link_flair_text,\n",
    "                'type': 'post',\n",
    "                'parent_id': None,\n",
    "                'subreddit': subreddit_name\n",
    "            }\n",
    "            new_data.append(post_data)\n",
    "            comments = get_comments(post)\n",
    "            for comment in comments:\n",
    "                comment_data = {\n",
    "                    'title': None,\n",
    "                    'text': comment.body,\n",
    "                    'upvotes': comment.score,\n",
    "                    'author': str(comment.author),\n",
    "                    'date': dt.datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'id': comment.id,\n",
    "                    'flair': None,\n",
    "                    'type': 'comment',\n",
    "                    'parent_id': comment.parent_id,\n",
    "                    'subreddit': subreddit_name\n",
    "                }\n",
    "                new_data.append(comment_data)\n",
    "\n",
    "    return new_data\n",
    "\n",
    "# Load existing data if available\n",
    "try:\n",
    "    existing_data = pd.read_csv('reddit_data_combined.csv')\n",
    "except FileNotFoundError:\n",
    "    existing_data = pd.DataFrame()\n",
    "\n",
    "# Function to run the data scraping and saving\n",
    "def run_data_collection():\n",
    "    print(f\"Scraping data for {dt.datetime.now().strftime('%Y-%m-%d')}\")\n",
    "    daily_data = scrape_data()\n",
    "    df_daily = pd.DataFrame(daily_data)\n",
    "\n",
    "    # Combine with existing data and remove duplicates\n",
    "    combined_data = pd.concat([existing_data, df_daily]).drop_duplicates(subset='id').reset_index(drop=True)\n",
    "\n",
    "    # Save the combined data\n",
    "    combined_data.to_csv('reddit_data_combined.csv', index=False)\n",
    "    print(f\"Data scraped and saved to reddit_data_combined.csv\")\n",
    "\n",
    "# Run the data collection function directly for now\n",
    "# In production, you might want to use a scheduler as before\n",
    "run_data_collection()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is for the"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while scraping the article body: 'str' object has no attribute 'find_element'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[83], line 59\u001B[0m\n\u001B[0;32m     57\u001B[0m     full_link \u001B[38;5;241m=\u001B[39m requests\u001B[38;5;241m.\u001B[39mcompat\u001B[38;5;241m.\u001B[39murljoin(main_url, link)\n\u001B[0;32m     58\u001B[0m     body \u001B[38;5;241m=\u001B[39m scrape_article_body(full_link)\n\u001B[1;32m---> 59\u001B[0m     comments \u001B[38;5;241m=\u001B[39m \u001B[43mscrape_comments\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfull_link\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     60\u001B[0m     articles\u001B[38;5;241m.\u001B[39mappend({\n\u001B[0;32m     61\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtitle\u001B[39m\u001B[38;5;124m'\u001B[39m: title,\n\u001B[0;32m     62\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124murl\u001B[39m\u001B[38;5;124m'\u001B[39m: full_link,\n\u001B[0;32m     63\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbody\u001B[39m\u001B[38;5;124m'\u001B[39m: body,\n\u001B[0;32m     64\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcomments\u001B[39m\u001B[38;5;124m'\u001B[39m: comments\n\u001B[0;32m     65\u001B[0m     })\n\u001B[0;32m     67\u001B[0m \u001B[38;5;66;03m# File path for the CSV file\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[83], line 29\u001B[0m, in \u001B[0;36mscrape_comments\u001B[1;34m(article_url)\u001B[0m\n\u001B[0;32m     27\u001B[0m time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# Politeness policy: wait 1 second between requests\u001B[39;00m\n\u001B[0;32m     28\u001B[0m response \u001B[38;5;241m=\u001B[39m requests\u001B[38;5;241m.\u001B[39mget(article_url, headers\u001B[38;5;241m=\u001B[39mheaders)\n\u001B[1;32m---> 29\u001B[0m article_soup \u001B[38;5;241m=\u001B[39m \u001B[43mBeautifulSoup\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mhtml.parser\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     30\u001B[0m comments \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     31\u001B[0m comments_list \u001B[38;5;241m=\u001B[39m article_soup\u001B[38;5;241m.\u001B[39mfind(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mul\u001B[39m\u001B[38;5;124m'\u001B[39m, class_\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspcv_messages-list\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\Documents\\Uni\\MA5851 Data Science Master Class 1\\Assignments\\A3_Capstone\\A3_JakeLasslett_2\\lib\\site-packages\\bs4\\__init__.py:335\u001B[0m, in \u001B[0;36mBeautifulSoup.__init__\u001B[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001B[0m\n\u001B[0;32m    333\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuilder\u001B[38;5;241m.\u001B[39minitialize_soup(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m    334\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 335\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_feed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    336\u001B[0m     success \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    337\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\Uni\\MA5851 Data Science Master Class 1\\Assignments\\A3_Capstone\\A3_JakeLasslett_2\\lib\\site-packages\\bs4\\__init__.py:478\u001B[0m, in \u001B[0;36mBeautifulSoup._feed\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    475\u001B[0m \u001B[38;5;66;03m# Convert the document to Unicode.\u001B[39;00m\n\u001B[0;32m    476\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuilder\u001B[38;5;241m.\u001B[39mreset()\n\u001B[1;32m--> 478\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeed\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmarkup\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    479\u001B[0m \u001B[38;5;66;03m# Close out any unfinished strings and close all the open tags.\u001B[39;00m\n\u001B[0;32m    480\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mendData()\n",
      "File \u001B[1;32m~\\Documents\\Uni\\MA5851 Data Science Master Class 1\\Assignments\\A3_Capstone\\A3_JakeLasslett_2\\lib\\site-packages\\bs4\\builder\\_htmlparser.py:380\u001B[0m, in \u001B[0;36mHTMLParserTreeBuilder.feed\u001B[1;34m(self, markup)\u001B[0m\n\u001B[0;32m    378\u001B[0m parser\u001B[38;5;241m.\u001B[39msoup \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msoup\n\u001B[0;32m    379\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 380\u001B[0m     \u001B[43mparser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmarkup\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    381\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    382\u001B[0m     \u001B[38;5;66;03m# html.parser raises AssertionError in rare cases to\u001B[39;00m\n\u001B[0;32m    383\u001B[0m     \u001B[38;5;66;03m# indicate a fatal problem with the markup, especially\u001B[39;00m\n\u001B[0;32m    384\u001B[0m     \u001B[38;5;66;03m# when there's an error in the doctype declaration.\u001B[39;00m\n\u001B[0;32m    385\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ParserRejectedMarkup(e)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\html\\parser.py:110\u001B[0m, in \u001B[0;36mHTMLParser.feed\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m    104\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Feed data to the parser.\u001B[39;00m\n\u001B[0;32m    105\u001B[0m \n\u001B[0;32m    106\u001B[0m \u001B[38;5;124;03mCall this as often as you want, with as little or as much text\u001B[39;00m\n\u001B[0;32m    107\u001B[0m \u001B[38;5;124;03mas you want (may include '\\n').\u001B[39;00m\n\u001B[0;32m    108\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    109\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrawdata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrawdata \u001B[38;5;241m+\u001B[39m data\n\u001B[1;32m--> 110\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgoahead\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\html\\parser.py:172\u001B[0m, in \u001B[0;36mHTMLParser.goahead\u001B[1;34m(self, end)\u001B[0m\n\u001B[0;32m    170\u001B[0m     k \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparse_starttag(i)\n\u001B[0;32m    171\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m startswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m</\u001B[39m\u001B[38;5;124m\"\u001B[39m, i):\n\u001B[1;32m--> 172\u001B[0m     k \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparse_endtag\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    173\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m startswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<!--\u001B[39m\u001B[38;5;124m\"\u001B[39m, i):\n\u001B[0;32m    174\u001B[0m     k \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparse_comment(i)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\html\\parser.py:420\u001B[0m, in \u001B[0;36mHTMLParser.parse_endtag\u001B[1;34m(self, i)\u001B[0m\n\u001B[0;32m    417\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_data(rawdata[i:gtpos])\n\u001B[0;32m    418\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m gtpos\n\u001B[1;32m--> 420\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_endtag\u001B[49m\u001B[43m(\u001B[49m\u001B[43melem\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    421\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclear_cdata_mode()\n\u001B[0;32m    422\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m gtpos\n",
      "File \u001B[1;32m~\\Documents\\Uni\\MA5851 Data Science Master Class 1\\Assignments\\A3_Capstone\\A3_JakeLasslett_2\\lib\\site-packages\\bs4\\builder\\_htmlparser.py:176\u001B[0m, in \u001B[0;36mBeautifulSoupHTMLParser.handle_endtag\u001B[1;34m(self, name, check_already_closed)\u001B[0m\n\u001B[0;32m    174\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39malready_closed_empty_element\u001B[38;5;241m.\u001B[39mremove(name)\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 176\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msoup\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_endtag\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Uni\\MA5851 Data Science Master Class 1\\Assignments\\A3_Capstone\\A3_JakeLasslett_2\\lib\\site-packages\\bs4\\__init__.py:770\u001B[0m, in \u001B[0;36mBeautifulSoup.handle_endtag\u001B[1;34m(self, name, nsprefix)\u001B[0m\n\u001B[0;32m    764\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Called by the tree builder when an ending tag is encountered.\u001B[39;00m\n\u001B[0;32m    765\u001B[0m \n\u001B[0;32m    766\u001B[0m \u001B[38;5;124;03m:param name: Name of the tag.\u001B[39;00m\n\u001B[0;32m    767\u001B[0m \u001B[38;5;124;03m:param nsprefix: Namespace prefix for the tag.\u001B[39;00m\n\u001B[0;32m    768\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    769\u001B[0m \u001B[38;5;66;03m#print(\"End tag: \" + name)\u001B[39;00m\n\u001B[1;32m--> 770\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mendData\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    771\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_popToTag(name, nsprefix)\n",
      "File \u001B[1;32m~\\Documents\\Uni\\MA5851 Data Science Master Class 1\\Assignments\\A3_Capstone\\A3_JakeLasslett_2\\lib\\site-packages\\bs4\\__init__.py:618\u001B[0m, in \u001B[0;36mBeautifulSoup.endData\u001B[1;34m(self, containerClass)\u001B[0m\n\u001B[0;32m    616\u001B[0m containerClass \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstring_container(containerClass)\n\u001B[0;32m    617\u001B[0m o \u001B[38;5;241m=\u001B[39m containerClass(current_data)\n\u001B[1;32m--> 618\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobject_was_parsed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mo\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Uni\\MA5851 Data Science Master Class 1\\Assignments\\A3_Capstone\\A3_JakeLasslett_2\\lib\\site-packages\\bs4\\__init__.py:639\u001B[0m, in \u001B[0;36mBeautifulSoup.object_was_parsed\u001B[1;34m(self, o, parent, most_recent_element)\u001B[0m\n\u001B[0;32m    635\u001B[0m         previous_element \u001B[38;5;241m=\u001B[39m o\u001B[38;5;241m.\u001B[39mprevious_element\n\u001B[0;32m    637\u001B[0m fix \u001B[38;5;241m=\u001B[39m parent\u001B[38;5;241m.\u001B[39mnext_element \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 639\u001B[0m \u001B[43mo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msetup\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprevious_element\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnext_element\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprevious_sibling\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnext_sibling\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    641\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_most_recent_element \u001B[38;5;241m=\u001B[39m o\n\u001B[0;32m    642\u001B[0m parent\u001B[38;5;241m.\u001B[39mcontents\u001B[38;5;241m.\u001B[39mappend(o)\n",
      "File \u001B[1;32m~\\Documents\\Uni\\MA5851 Data Science Master Class 1\\Assignments\\A3_Capstone\\A3_JakeLasslett_2\\lib\\site-packages\\bs4\\element.py:156\u001B[0m, in \u001B[0;36mPageElement.setup\u001B[1;34m(self, parent, previous_element, next_element, previous_sibling, next_sibling)\u001B[0m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;66;03m# In general, we can't tell just by looking at an element whether\u001B[39;00m\n\u001B[0;32m    152\u001B[0m \u001B[38;5;66;03m# it's contained in an XML document or an HTML document. But for\u001B[39;00m\n\u001B[0;32m    153\u001B[0m \u001B[38;5;66;03m# Tags (q.v.) we can store this information at parse time.\u001B[39;00m\n\u001B[0;32m    154\u001B[0m known_xml \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 156\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msetup\u001B[39m(\u001B[38;5;28mself\u001B[39m, parent\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, previous_element\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, next_element\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    157\u001B[0m           previous_sibling\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, next_sibling\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    158\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Sets up the initial relations between this element and\u001B[39;00m\n\u001B[0;32m    159\u001B[0m \u001B[38;5;124;03m    other elements.\u001B[39;00m\n\u001B[0;32m    160\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    173\u001B[0m \u001B[38;5;124;03m        on the same level of the parse tree as this one.\u001B[39;00m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m    175\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparent \u001B[38;5;241m=\u001B[39m parent\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# # Headers to mimic a browser visit\n",
    "# headers = {\n",
    "#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "# }\n",
    "#\n",
    "# # Function to scrape the body of an article\n",
    "# def scrape_article_body(driver):\n",
    "#     try:\n",
    "#         # Wait for the content to be visible, not just present\n",
    "#         WebDriverWait(driver, 30).until(\n",
    "#             EC.visibility_of_element_located((By.CSS_SELECTOR, \"div.wysiwyg.wysiwyg--all-content.css-1bkk12\"))\n",
    "#         )\n",
    "#         html_source = driver.page_source\n",
    "#         article_soup = BeautifulSoup(html_source, 'html.parser')\n",
    "#         content_container = article_soup.find('div', class_='wysiwyg wysiwyg--all-content css-1bkk12')\n",
    "#         paragraphs = content_container.find_all('p')\n",
    "#         return ' '.join(paragraph.get_text(strip=True) for paragraph in paragraphs)\n",
    "#     except TimeoutException:\n",
    "#         print(\"Timed out waiting for page to load\")\n",
    "#         return 'Content loading timed out'\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred while scraping the article body: {e}\")\n",
    "#         return 'Error while scraping'\n",
    "#\n",
    "#  # Function to scrape comments from an article\n",
    "# def scrape_comments(article_url):\n",
    "#     time.sleep(1)  # Politeness policy: wait 1 second between requests\n",
    "#     response = requests.get(article_url, headers=headers)\n",
    "#     article_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#     comments = []\n",
    "#     comments_list = article_soup.find('ul', class_='spcv_messages-list')\n",
    "#     if comments_list:\n",
    "#         comment_items = comments_list.find_all('li', class_='spcv_list-item')\n",
    "#         for comment_item in comment_items:\n",
    "#             # Extract the text content of the comment. The structure suggests it might be within a 'p' or 'div' tag.\n",
    "#             comment_text = comment_item.find(['p', 'div'], recursive=True).get_text(strip=True)\n",
    "#             comments.append(comment_text)\n",
    "#     return comments\n",
    "#\n",
    "# # URL of the main page\n",
    "# main_url = 'https://www.jpost.com/Tags/israeli-palestinian-conflict'\n",
    "#\n",
    "# # Fetch the content from the main URL\n",
    "# response = requests.get(main_url, headers=headers)\n",
    "# html_content = response.content\n",
    "#\n",
    "# # Parse the HTML content of the main page\n",
    "# soup = BeautifulSoup(html_content, 'html.parser')\n",
    "#\n",
    "# # Extract article titles, URLs, bodies, and comments\n",
    "# articles = []\n",
    "# for article in soup.find_all('div', class_='itc-info'):\n",
    "#     title_tag = article.find('h3', class_='itc-info-title')\n",
    "#     title = title_tag.get_text(strip=True) if title_tag else 'No title'\n",
    "#     link_tag = article.find('a')\n",
    "#     link = link_tag['href'] if link_tag else 'No URL'\n",
    "#     full_link = requests.compat.urljoin(main_url, link)\n",
    "#     body = scrape_article_body(full_link)\n",
    "#     comments = scrape_comments(full_link)\n",
    "#     articles.append({\n",
    "#         'title': title,\n",
    "#         'url': full_link,\n",
    "#         'body': body,\n",
    "#         'comments': comments\n",
    "#     })\n",
    "#\n",
    "# # File path for the CSV file\n",
    "# csv_file = 'articles_with_comments.csv'\n",
    "#\n",
    "# # Check if the file exists\n",
    "# if os.path.exists(csv_file):\n",
    "#     existing_df = pd.read_csv(csv_file)\n",
    "# else:\n",
    "#     existing_df = pd.DataFrame()\n",
    "#\n",
    "# # Convert the list of dictionaries to a DataFrame\n",
    "# df_articles = pd.DataFrame(articles)\n",
    "#\n",
    "# # Append the new data to the existing DataFrame\n",
    "# updated_df = pd.concat([existing_df, df_articles], ignore_index=True)\n",
    "#\n",
    "# # Display the updated DataFrame\n",
    "# print(updated_df.head())  # Show the first few rows of the DataFrame\n",
    "#\n",
    "# # Save the updated DataFrame to a CSV file\n",
    "# updated_df.to_csv(csv_file, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "WEBSCRAPER 2: AL JAZEERA:\n",
    "This is for https://www.aljazeera.com/tag/israel-palestine-conflict/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-08 21:12:36,858 - INFO - No overlay found or not clickable.\n",
      "2023-12-08 21:12:37,036 - INFO - Loaded existing data.\n",
      "2023-12-08 21:12:37,131 - INFO - Found 14 articles on the page.\n",
      "2023-12-08 21:12:42,295 - INFO - Found 24 articles on the page.\n",
      "2023-12-08 21:12:47,451 - INFO - Found 34 articles on the page.\n",
      "2023-12-08 21:12:53,643 - INFO - Found 44 articles on the page.\n",
      "2023-12-08 21:12:58,824 - INFO - Found 54 articles on the page.\n",
      "2023-12-08 21:13:03,998 - INFO - Found 64 articles on the page.\n",
      "2023-12-08 21:13:09,193 - INFO - Found 74 articles on the page.\n",
      "2023-12-08 21:13:14,391 - INFO - Found 84 articles on the page.\n",
      "2023-12-08 21:13:19,603 - INFO - Found 94 articles on the page.\n",
      "2023-12-08 21:13:24,837 - INFO - Found 104 articles on the page.\n",
      "2023-12-08 21:13:30,067 - INFO - Found 114 articles on the page.\n",
      "2023-12-08 21:13:35,481 - INFO - Found 124 articles on the page.\n",
      "2023-12-08 21:13:40,734 - INFO - Found 134 articles on the page.\n",
      "2023-12-08 21:13:45,972 - INFO - Found 144 articles on the page.\n",
      "2023-12-08 21:13:51,205 - INFO - Found 154 articles on the page.\n",
      "2023-12-08 21:13:56,464 - INFO - Found 164 articles on the page.\n",
      "2023-12-08 21:14:01,966 - INFO - Found 174 articles on the page.\n",
      "2023-12-08 21:14:07,258 - INFO - Found 184 articles on the page.\n",
      "2023-12-08 21:14:12,527 - INFO - Found 194 articles on the page.\n",
      "2023-12-08 21:14:17,832 - INFO - Found 204 articles on the page.\n",
      "2023-12-08 21:14:23,143 - INFO - Found 214 articles on the page.\n",
      "2023-12-08 21:14:28,605 - INFO - Found 224 articles on the page.\n",
      "2023-12-08 21:14:33,947 - INFO - Found 234 articles on the page.\n",
      "2023-12-08 21:14:39,288 - INFO - Found 244 articles on the page.\n",
      "2023-12-08 21:14:44,768 - INFO - Found 254 articles on the page.\n",
      "2023-12-08 21:14:50,153 - INFO - Found 264 articles on the page.\n",
      "2023-12-08 21:14:55,620 - INFO - Found 274 articles on the page.\n",
      "2023-12-08 21:15:01,314 - INFO - Found 284 articles on the page.\n",
      "2023-12-08 21:15:06,734 - INFO - Found 294 articles on the page.\n",
      "2023-12-08 21:15:12,134 - INFO - Found 304 articles on the page.\n",
      "2023-12-08 21:15:17,681 - INFO - Found 314 articles on the page.\n",
      "2023-12-08 21:29:22,189 - WARNING - Timed out trying to load https://www.aljazeera.com/news/2023/12/8/alarm-sirens-explosions-heard-near-us-embassy-in-baghdad\n",
      "2023-12-08 21:46:13,947 - WARNING - Timed out trying to load https://www.aljazeera.com/gallery/2023/11/27/israel-hamas-and-the-toil-of-the-prisoner-exchange\n",
      "2023-12-08 21:57:20,259 - INFO - New articles added.\n",
      "2023-12-08 21:57:20,370 - INFO - Data saved to data_aljazeera_withdates.csv.\n"
     ]
    }
   ],
   "source": [
    "# Function to set up logging\n",
    "def setup_logging():\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Function to scrape the body of an article and the publication date\n",
    "def scrape_article_body_and_date(driver):\n",
    "    try:\n",
    "        WebDriverWait(driver, 30).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"#main-content-area p\"))\n",
    "        )\n",
    "        html_source = driver.page_source\n",
    "        article_soup = BeautifulSoup(html_source, 'html.parser')\n",
    "\n",
    "        # Extracting the article body\n",
    "        content_container = article_soup.select_one(\"#main-content-area\")\n",
    "        paragraphs = content_container.find_all('p')\n",
    "        article_body = ' '.join(paragraph.get_text(strip=True) for paragraph in paragraphs)\n",
    "\n",
    "        # Extracting the article date\n",
    "        date_element = article_soup.select_one(\".article-dates .date-simple span\")\n",
    "        article_date = date_element.get_text(strip=True) if date_element else 'Date not found'\n",
    "\n",
    "        return article_body, article_date\n",
    "    except TimeoutException:\n",
    "        logging.warning(\"Timed out waiting for page to load\")\n",
    "        return 'Content loading timed out', ''\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while scraping the article body: {e}\")\n",
    "        return 'Error while scraping', ''\n",
    "\n",
    "def get_article_urls_and_titles(driver, visited_urls):\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    new_articles = []\n",
    "    articles = soup.find_all('article')\n",
    "    logging.info(f\"Found {len(articles)} articles on the page.\")\n",
    "    for article in articles:\n",
    "        title_element = article.find('h3')\n",
    "        link_element = article.find('a', href=True)\n",
    "        if title_element and link_element:\n",
    "            title = title_element.get_text(strip=True)\n",
    "            url = urljoin(base_url, link_element['href'])\n",
    "            if url not in visited_urls:\n",
    "                visited_urls.add(url)\n",
    "                new_articles.append({'title': title, 'url': url})\n",
    "    return new_articles\n",
    "\n",
    "def click_show_more(driver):\n",
    "    try:\n",
    "        show_more_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.show-more-button\"))\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", show_more_button)\n",
    "        show_more_button.click()\n",
    "        return True\n",
    "    except TimeoutException:\n",
    "        logging.info(\"No 'Show more' button to click or timed out waiting for the button to be clickable.\")\n",
    "        return False\n",
    "\n",
    "setup_logging()\n",
    "\n",
    "# Set up Chrome options to run the browser visibly (not headless)\n",
    "options = Options()\n",
    "options.headless = False  # Disable headless mode to visualize the browser actions\n",
    "options.add_argument(\"--window-size=1920,1200\")\n",
    "\n",
    "# Instantiate a new driver\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "base_url = 'https://www.aljazeera.com'\n",
    "driver.get(base_url + '/tag/israel-palestine-conflict/')\n",
    "\n",
    "# Wait for the initial articles to load\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"article\")))\n",
    "\n",
    "# Attempt to close any overlaying elements like cookie consent banners\n",
    "try:\n",
    "    close_button = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.close, div.cookie-consent button, a[href*='cookie']\"))\n",
    "    )\n",
    "    close_button.click()\n",
    "except (TimeoutException, NoSuchElementException, ElementClickInterceptedException):\n",
    "    logging.info(\"No overlay found or not clickable.\")\n",
    "\n",
    "# Initialize a set for visited URLs\n",
    "visited_urls = set()\n",
    "\n",
    "# Load existing data\n",
    "csv_file = 'data_aljazeera_withdates.csv'\n",
    "try:\n",
    "    existing_data = pd.read_csv(csv_file)\n",
    "    existing_urls = set(existing_data['url'])\n",
    "    logging.info(\"Loaded existing data.\")\n",
    "except FileNotFoundError:\n",
    "    existing_data = pd.DataFrame()\n",
    "    existing_urls = set()\n",
    "    logging.info(\"No existing file found. Creating a new dataset.\")\n",
    "\n",
    "# Scrape the initial set of articles\n",
    "all_articles = get_article_urls_and_titles(driver, visited_urls)\n",
    "\n",
    "# Click the 'Show more' button to reveal more articles\n",
    "for _ in range(30):\n",
    "    if not click_show_more(driver):\n",
    "        break\n",
    "    time.sleep(5)  # Wait for the page to load more articles\n",
    "    all_articles.extend(get_article_urls_and_titles(driver, visited_urls))\n",
    "\n",
    "# Now visit each URL and get the body content and date\n",
    "new_articles = []\n",
    "for article in all_articles:\n",
    "    if article['url'] not in existing_urls:\n",
    "        try:\n",
    "            driver.get(article['url'])\n",
    "            time.sleep(2)  # Ensure the page has loaded\n",
    "            article_body, article_date = scrape_article_body_and_date(driver)\n",
    "            article['body'] = article_body\n",
    "            article['date'] = article_date\n",
    "            new_articles.append(article)\n",
    "        except TimeoutException:\n",
    "            logging.warning(f\"Timed out trying to load {article['url']}\")\n",
    "            continue\n",
    "\n",
    "# Combine new articles with existing data\n",
    "if new_articles:\n",
    "    df_new_articles = pd.DataFrame(new_articles)\n",
    "    combined_data = pd.concat([existing_data, df_new_articles], ignore_index=True)\n",
    "    logging.info(\"New articles added.\")\n",
    "else:\n",
    "    combined_data = existing_data\n",
    "    logging.info(\"No new articles to add.\")\n",
    "\n",
    "# Save the combined data to a CSV file\n",
    "combined_data.to_csv(csv_file, index=False)\n",
    "logging.info(f\"Data saved to {csv_file}.\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is the scraper for news.com.au"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-08 19:21:55,096 - INFO - No existing file found. Creating a new dataset.\n",
      "2023-12-08 19:23:10,622 - INFO - Found 30 articles on the page.\n",
      "2023-12-08 19:23:22,545 - INFO - Found 30 articles on the page.\n",
      "2023-12-08 19:23:34,036 - INFO - Found 30 articles on the page.\n",
      "2023-12-08 19:23:45,492 - INFO - Found 30 articles on the page.\n",
      "2023-12-08 19:23:58,863 - INFO - Found 30 articles on the page.\n",
      "2023-12-08 19:24:11,259 - INFO - Found 30 articles on the page.\n",
      "2023-12-08 19:24:23,129 - INFO - Found 30 articles on the page.\n",
      "2023-12-08 19:52:33,566 - INFO - New articles added.\n",
      "2023-12-08 19:52:33,674 - INFO - Data saved to data_newscomau_withdates.csv.\n",
      "2023-12-08 19:52:37,846 - INFO - Scraping completed.\n"
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Function to scrape the body of an article and the publication date\n",
    "def scrape_article_body_newscomau(driver):\n",
    "    try:\n",
    "        WebDriverWait(driver, 30).until(\n",
    "            EC.presence_of_element_located((By.ID, \"story-body\"))\n",
    "        )\n",
    "        html_source = driver.page_source\n",
    "        article_soup = BeautifulSoup(html_source, 'html.parser')\n",
    "\n",
    "        # Extracting the article body\n",
    "        content_container = article_soup.find(id=\"story-body\")\n",
    "        paragraphs = content_container.find_all('p')\n",
    "        article_body = ' '.join(paragraph.get_text(strip=True) for paragraph in paragraphs)\n",
    "\n",
    "        # Extracting the article date\n",
    "        date_element = article_soup.find('div', class_='byline_publish')\n",
    "        article_date = date_element.get_text(strip=True) if date_element else 'Date not found'\n",
    "\n",
    "        return article_body, article_date\n",
    "    except TimeoutException:\n",
    "        logging.warning(\"Timed out waiting for the article's body to load\")\n",
    "        return 'Content loading timed out', ''\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while scraping the article body: {e}\")\n",
    "        return 'Error while scraping', ''\n",
    "\n",
    "# Define the function to get article URLs and titles\n",
    "def get_article_urls_and_titles_newscomau(driver, visited_urls):\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    new_articles = []\n",
    "    articles = soup.find_all('article', class_='storyblock')\n",
    "    logging.info(f\"Found {len(articles)} articles on the page.\")\n",
    "    for article in articles:\n",
    "        title_element = article.find('a', class_='storyblock_title_link')\n",
    "        if title_element:\n",
    "            title = title_element.get_text(strip=True)\n",
    "            url = urljoin('https://www.news.com.au', title_element['href'])\n",
    "            if url not in visited_urls:\n",
    "                visited_urls.add(url)\n",
    "                new_articles.append({'title': title, 'url': url})\n",
    "    return new_articles\n",
    "\n",
    "# Set up Chrome options to run the browser visibly (not headless)\n",
    "options = Options()\n",
    "options.headless = False\n",
    "options.add_argument(\"--window-size=1920,1200\")\n",
    "\n",
    "# Instantiate a new driver\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Initialize a set for visited URLs\n",
    "visited_urls = set()\n",
    "\n",
    "base_url = 'https://www.news.com.au/world/middle-east'\n",
    "\n",
    "# Load existing data if available\n",
    "csv_file = 'data_newscomau_withdates.csv'\n",
    "try:\n",
    "    existing_data = pd.read_csv(csv_file)\n",
    "    existing_urls = set(existing_data['url'])\n",
    "    logging.info(\"Loaded existing data.\")\n",
    "except FileNotFoundError:\n",
    "    existing_data = pd.DataFrame(columns=['title', 'url', 'body', 'date'])\n",
    "    existing_urls = set()\n",
    "    logging.info(\"No existing file found. Creating a new dataset.\")\n",
    "\n",
    "# Scrape articles from page 1 to page 6\n",
    "all_articles = []\n",
    "for i in range(1, 8):  # Page numbers 1 to 6\n",
    "    page_url = f\"{base_url}/page/{i}\" if i > 1 else base_url\n",
    "    driver.get(page_url)\n",
    "    time.sleep(5)  # Wait for the page to load completely\n",
    "    all_articles.extend(get_article_urls_and_titles_newscomau(driver, visited_urls))\n",
    "\n",
    "# Now visit each URL and get the body content and date\n",
    "new_articles = []\n",
    "for article in all_articles:\n",
    "    if article['url'] not in existing_urls:\n",
    "        driver.get(article['url'])\n",
    "        time.sleep(2)  # Ensure the page has loaded\n",
    "        article_body, article_date = scrape_article_body_newscomau(driver)\n",
    "        article['body'] = article_body\n",
    "        article['date'] = article_date\n",
    "        new_articles.append(article)\n",
    "\n",
    "# Combine new articles with existing data\n",
    "if new_articles:\n",
    "    df_new_articles = pd.DataFrame(new_articles)\n",
    "    combined_data = pd.concat([existing_data, df_new_articles], ignore_index=True)\n",
    "    logging.info(\"New articles added.\")\n",
    "else:\n",
    "    combined_data = existing_data\n",
    "    logging.info(\"No new articles to add.\")\n",
    "\n",
    "# Save the combined data to a CSV file\n",
    "combined_data.to_csv(csv_file, index=False)\n",
    "logging.info(f\"Data saved to {csv_file}.\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "logging.info(\"Scraping completed.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}