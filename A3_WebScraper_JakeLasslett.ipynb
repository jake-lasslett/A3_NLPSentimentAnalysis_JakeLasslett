{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "import datetime as dt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementClickInterceptedException\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is WebScraper for Reddit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "## This is pulling my credentials from a secret config.py file which will not be shared\n",
    "from config import client_id, client_secret, user_agent, password, username\n",
    "\n",
    "# Set up PRAW with your credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    password=password,\n",
    "    user_agent=user_agent,\n",
    "    username=username,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subreddits = ['N_N_N', 'CombatFootage', 'israelpalestine', 'AskMiddleEast', 'israelhamaswar']\n",
    "keywords = ['palestine', 'israel', 'gaza', 'west bank', 'hamas', 'idf', 'settlements']\n",
    "\n",
    "def is_relevant(text):\n",
    "    text_lower = text.lower()\n",
    "    return any(keyword in text_lower for keyword in keywords)\n",
    "\n",
    "def get_top_posts(subreddit_name):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts = []\n",
    "    for post in subreddit.top('day', limit=30):  # Fetches top 30 posts of the day\n",
    "        if is_relevant(post.title) or is_relevant(post.selftext): ## Searches for the relevent keywords in the title and text\n",
    "            posts.append(post)\n",
    "    return posts[:15]  # Limit to 15 relevent posts\n",
    "\n",
    "def get_comments(post):\n",
    "    post.comments.replace_more(limit=0)  # This line ensures that all MoreComments objects are removed\n",
    "    comments = post.comments.list()\n",
    "    return comments[:5]  # Limit to the first 5 comments\n",
    "\n",
    "def scrape_data():\n",
    "    new_data = []\n",
    "    for subreddit_name in subreddits:\n",
    "        top_posts = get_top_posts(subreddit_name)\n",
    "        for post in top_posts:\n",
    "            post_data = {\n",
    "                'title': post.title,\n",
    "                'text': post.selftext,\n",
    "                'upvotes': post.score,\n",
    "                'author': str(post.author),\n",
    "                'date': dt.datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'id': post.id,\n",
    "                'flair': post.link_flair_text,\n",
    "                'type': 'post',\n",
    "                'parent_id': None,\n",
    "                'subreddit': subreddit_name\n",
    "            }\n",
    "            new_data.append(post_data)\n",
    "            comments = get_comments(post)\n",
    "            for comment in comments:\n",
    "                comment_data = {\n",
    "                    'title': None,\n",
    "                    'text': comment.body,\n",
    "                    'upvotes': comment.score,\n",
    "                    'author': str(comment.author),\n",
    "                    'date': dt.datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'id': comment.id,\n",
    "                    'flair': None,\n",
    "                    'type': 'comment',\n",
    "                    'parent_id': comment.parent_id,\n",
    "                    'subreddit': subreddit_name\n",
    "                }\n",
    "                new_data.append(comment_data)\n",
    "\n",
    "    return new_data\n",
    "\n",
    "# Load existing data if available\n",
    "try:\n",
    "    existing_data = pd.read_csv('reddit_data_combined.csv')\n",
    "except FileNotFoundError:\n",
    "    existing_data = pd.DataFrame()\n",
    "\n",
    "# Function to run the data scraping and saving\n",
    "def run_data_collection():\n",
    "    print(f\"Scraping data for {dt.datetime.now().strftime('%Y-%m-%d')}\")\n",
    "    daily_data = scrape_data()\n",
    "    df_daily = pd.DataFrame(daily_data)\n",
    "\n",
    "    # Combine with existing data and remove duplicates\n",
    "    combined_data = pd.concat([existing_data, df_daily]).drop_duplicates(subset='id').reset_index(drop=True)\n",
    "\n",
    "    # Save the combined data\n",
    "    combined_data.to_csv('reddit_data_combined.csv', index=False)\n",
    "    print(f\"Data scraped and saved to reddit_data_combined.csv\")\n",
    "\n",
    "# Run the data collection function directly for now\n",
    "run_data_collection()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "WEBSCRAPER 2: AL JAZEERA:\n",
    "This is for https://www.aljazeera.com/tag/israel-palestine-conflict/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-08 21:12:36,858 - INFO - No overlay found or not clickable.\n",
      "2023-12-08 21:12:37,036 - INFO - Loaded existing data.\n",
      "2023-12-08 21:12:37,131 - INFO - Found 14 articles on the page.\n",
      "2023-12-08 21:12:42,295 - INFO - Found 24 articles on the page.\n",
      "2023-12-08 21:12:47,451 - INFO - Found 34 articles on the page.\n",
      "2023-12-08 21:12:53,643 - INFO - Found 44 articles on the page.\n",
      "2023-12-08 21:12:58,824 - INFO - Found 54 articles on the page.\n",
      "2023-12-08 21:13:03,998 - INFO - Found 64 articles on the page.\n",
      "2023-12-08 21:13:09,193 - INFO - Found 74 articles on the page.\n",
      "2023-12-08 21:13:14,391 - INFO - Found 84 articles on the page.\n",
      "2023-12-08 21:13:19,603 - INFO - Found 94 articles on the page.\n",
      "2023-12-08 21:13:24,837 - INFO - Found 104 articles on the page.\n",
      "2023-12-08 21:13:30,067 - INFO - Found 114 articles on the page.\n",
      "2023-12-08 21:13:35,481 - INFO - Found 124 articles on the page.\n",
      "2023-12-08 21:13:40,734 - INFO - Found 134 articles on the page.\n",
      "2023-12-08 21:13:45,972 - INFO - Found 144 articles on the page.\n",
      "2023-12-08 21:13:51,205 - INFO - Found 154 articles on the page.\n",
      "2023-12-08 21:13:56,464 - INFO - Found 164 articles on the page.\n",
      "2023-12-08 21:14:01,966 - INFO - Found 174 articles on the page.\n",
      "2023-12-08 21:14:07,258 - INFO - Found 184 articles on the page.\n",
      "2023-12-08 21:14:12,527 - INFO - Found 194 articles on the page.\n",
      "2023-12-08 21:14:17,832 - INFO - Found 204 articles on the page.\n",
      "2023-12-08 21:14:23,143 - INFO - Found 214 articles on the page.\n",
      "2023-12-08 21:14:28,605 - INFO - Found 224 articles on the page.\n",
      "2023-12-08 21:14:33,947 - INFO - Found 234 articles on the page.\n",
      "2023-12-08 21:14:39,288 - INFO - Found 244 articles on the page.\n",
      "2023-12-08 21:14:44,768 - INFO - Found 254 articles on the page.\n",
      "2023-12-08 21:14:50,153 - INFO - Found 264 articles on the page.\n",
      "2023-12-08 21:14:55,620 - INFO - Found 274 articles on the page.\n",
      "2023-12-08 21:15:01,314 - INFO - Found 284 articles on the page.\n",
      "2023-12-08 21:15:06,734 - INFO - Found 294 articles on the page.\n",
      "2023-12-08 21:15:12,134 - INFO - Found 304 articles on the page.\n",
      "2023-12-08 21:15:17,681 - INFO - Found 314 articles on the page.\n",
      "2023-12-08 21:29:22,189 - WARNING - Timed out trying to load https://www.aljazeera.com/news/2023/12/8/alarm-sirens-explosions-heard-near-us-embassy-in-baghdad\n",
      "2023-12-08 21:46:13,947 - WARNING - Timed out trying to load https://www.aljazeera.com/gallery/2023/11/27/israel-hamas-and-the-toil-of-the-prisoner-exchange\n",
      "2023-12-08 21:57:20,259 - INFO - New articles added.\n",
      "2023-12-08 21:57:20,370 - INFO - Data saved to data_aljazeera_withdates.csv.\n"
     ]
    }
   ],
   "source": [
    "# Function to set up logging\n",
    "def setup_logging():\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Function to scrape the body of an article and the publication date\n",
    "def scrape_article_body_and_date(driver):\n",
    "    try:\n",
    "        WebDriverWait(driver, 30).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"#main-content-area p\"))\n",
    "        )\n",
    "        html_source = driver.page_source\n",
    "        article_soup = BeautifulSoup(html_source, 'html.parser')\n",
    "\n",
    "        # Extracting the article body\n",
    "        content_container = article_soup.select_one(\"#main-content-area\")\n",
    "        paragraphs = content_container.find_all('p')\n",
    "        article_body = ' '.join(paragraph.get_text(strip=True) for paragraph in paragraphs)\n",
    "\n",
    "        # Extracting the article date\n",
    "        date_element = article_soup.select_one(\".article-dates .date-simple span\")\n",
    "        article_date = date_element.get_text(strip=True) if date_element else 'Date not found'\n",
    "\n",
    "        return article_body, article_date\n",
    "    except TimeoutException:\n",
    "        logging.warning(\"Timed out waiting for page to load\")\n",
    "        return 'Content loading timed out', ''\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while scraping the article body: {e}\")\n",
    "        return 'Error while scraping', ''\n",
    "\n",
    "def get_article_urls_and_titles(driver, visited_urls):\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    new_articles = []\n",
    "    articles = soup.find_all('article')\n",
    "    logging.info(f\"Found {len(articles)} articles on the page.\")\n",
    "    for article in articles:\n",
    "        title_element = article.find('h3')\n",
    "        link_element = article.find('a', href=True)\n",
    "        if title_element and link_element:\n",
    "            title = title_element.get_text(strip=True)\n",
    "            url = urljoin(base_url, link_element['href'])\n",
    "            if url not in visited_urls:\n",
    "                visited_urls.add(url)\n",
    "                new_articles.append({'title': title, 'url': url})\n",
    "    return new_articles\n",
    "\n",
    "def click_show_more(driver):\n",
    "    try:\n",
    "        show_more_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.show-more-button\"))\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", show_more_button)\n",
    "        show_more_button.click()\n",
    "        return True\n",
    "    except TimeoutException:\n",
    "        logging.info(\"No 'Show more' button to click or timed out waiting for the button to be clickable.\")\n",
    "        return False\n",
    "\n",
    "setup_logging()\n",
    "\n",
    "# Set up Chrome options to run the browser visibly (not headless)\n",
    "options = Options()\n",
    "options.headless = False  # Disable headless mode to visualize the browser actions\n",
    "options.add_argument(\"--window-size=1920,1200\")\n",
    "\n",
    "# Instantiate a new driver\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "base_url = 'https://www.aljazeera.com'\n",
    "driver.get(base_url + '/tag/israel-palestine-conflict/')\n",
    "\n",
    "# Wait for the initial articles to load\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"article\")))\n",
    "\n",
    "# Attempt to close any overlaying elements like cookie consent banners\n",
    "try:\n",
    "    close_button = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.close, div.cookie-consent button, a[href*='cookie']\"))\n",
    "    )\n",
    "    close_button.click()\n",
    "except (TimeoutException, NoSuchElementException, ElementClickInterceptedException):\n",
    "    logging.info(\"No overlay found or not clickable.\")\n",
    "\n",
    "# Initialize a set for visited URLs\n",
    "visited_urls = set()\n",
    "\n",
    "# Load existing data\n",
    "csv_file = 'data_aljazeera_withdates.csv'\n",
    "try:\n",
    "    existing_data = pd.read_csv(csv_file)\n",
    "    existing_urls = set(existing_data['url'])\n",
    "    logging.info(\"Loaded existing data.\")\n",
    "except FileNotFoundError:\n",
    "    existing_data = pd.DataFrame()\n",
    "    existing_urls = set()\n",
    "    logging.info(\"No existing file found. Creating a new dataset.\")\n",
    "\n",
    "# Scrape the initial set of articles\n",
    "all_articles = get_article_urls_and_titles(driver, visited_urls)\n",
    "\n",
    "# Click the 'Show more' button to reveal more articles\n",
    "for _ in range(30):\n",
    "    if not click_show_more(driver):\n",
    "        break\n",
    "    time.sleep(5)  # Wait for the page to load more articles\n",
    "    all_articles.extend(get_article_urls_and_titles(driver, visited_urls))\n",
    "\n",
    "# Now visit each URL and get the body content and date\n",
    "new_articles = []\n",
    "for article in all_articles:\n",
    "    if article['url'] not in existing_urls:\n",
    "        try:\n",
    "            driver.get(article['url'])\n",
    "            time.sleep(2)  # Ensure the page has loaded\n",
    "            article_body, article_date = scrape_article_body_and_date(driver)\n",
    "            article['body'] = article_body\n",
    "            article['date'] = article_date\n",
    "            new_articles.append(article)\n",
    "        except TimeoutException:\n",
    "            logging.warning(f\"Timed out trying to load {article['url']}\")\n",
    "            continue\n",
    "\n",
    "# Combine new articles with existing data\n",
    "if new_articles:\n",
    "    df_new_articles = pd.DataFrame(new_articles)\n",
    "    combined_data = pd.concat([existing_data, df_new_articles], ignore_index=True)\n",
    "    logging.info(\"New articles added.\")\n",
    "else:\n",
    "    combined_data = existing_data\n",
    "    logging.info(\"No new articles to add.\")\n",
    "\n",
    "# Save the combined data to a CSV file\n",
    "combined_data.to_csv(csv_file, index=False)\n",
    "logging.info(f\"Data saved to {csv_file}.\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is the scraper for news.com.au"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-08 19:21:55,096 - INFO - No existing file found. Creating a new dataset.\n",
      "2023-12-08 19:23:10,622 - INFO - Found 30 articles on the page.\n",
      "2023-12-08 19:23:22,545 - INFO - Found 30 articles on the page.\n",
      "2023-12-08 19:23:34,036 - INFO - Found 30 articles on the page.\n",
      "2023-12-08 19:23:45,492 - INFO - Found 30 articles on the page.\n",
      "2023-12-08 19:23:58,863 - INFO - Found 30 articles on the page.\n",
      "2023-12-08 19:24:11,259 - INFO - Found 30 articles on the page.\n",
      "2023-12-08 19:24:23,129 - INFO - Found 30 articles on the page.\n",
      "2023-12-08 19:52:33,566 - INFO - New articles added.\n",
      "2023-12-08 19:52:33,674 - INFO - Data saved to data_newscomau_withdates.csv.\n",
      "2023-12-08 19:52:37,846 - INFO - Scraping completed.\n"
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Function to scrape the body of an article and the publication date\n",
    "def scrape_article_body_newscomau(driver):\n",
    "    try:\n",
    "        WebDriverWait(driver, 30).until(\n",
    "            EC.presence_of_element_located((By.ID, \"story-body\"))\n",
    "        )\n",
    "        html_source = driver.page_source\n",
    "        article_soup = BeautifulSoup(html_source, 'html.parser')\n",
    "\n",
    "        # Extracting the article body\n",
    "        content_container = article_soup.find(id=\"story-body\")\n",
    "        paragraphs = content_container.find_all('p')\n",
    "        article_body = ' '.join(paragraph.get_text(strip=True) for paragraph in paragraphs)\n",
    "\n",
    "        # Extracting the article date\n",
    "        date_element = article_soup.find('div', class_='byline_publish')\n",
    "        article_date = date_element.get_text(strip=True) if date_element else 'Date not found'\n",
    "\n",
    "        return article_body, article_date\n",
    "    except TimeoutException:\n",
    "        logging.warning(\"Timed out waiting for the article's body to load\")\n",
    "        return 'Content loading timed out', ''\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while scraping the article body: {e}\")\n",
    "        return 'Error while scraping', ''\n",
    "\n",
    "# Define the function to get article URLs and titles\n",
    "def get_article_urls_and_titles_newscomau(driver, visited_urls):\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    new_articles = []\n",
    "    articles = soup.find_all('article', class_='storyblock')\n",
    "    logging.info(f\"Found {len(articles)} articles on the page.\")\n",
    "    for article in articles:\n",
    "        title_element = article.find('a', class_='storyblock_title_link')\n",
    "        if title_element:\n",
    "            title = title_element.get_text(strip=True)\n",
    "            url = urljoin('https://www.news.com.au', title_element['href'])\n",
    "            if url not in visited_urls:\n",
    "                visited_urls.add(url)\n",
    "                new_articles.append({'title': title, 'url': url})\n",
    "    return new_articles\n",
    "\n",
    "# Set up Chrome options to run the browser visibly (not headless)\n",
    "options = Options()\n",
    "options.headless = False\n",
    "options.add_argument(\"--window-size=1920,1200\")\n",
    "\n",
    "# Instantiate a new driver\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Initialize a set for visited URLs\n",
    "visited_urls = set()\n",
    "\n",
    "base_url = 'https://www.news.com.au/world/middle-east'\n",
    "\n",
    "# Load existing data if available\n",
    "csv_file = 'data_newscomau_withdates.csv'\n",
    "try:\n",
    "    existing_data = pd.read_csv(csv_file)\n",
    "    existing_urls = set(existing_data['url'])\n",
    "    logging.info(\"Loaded existing data.\")\n",
    "except FileNotFoundError:\n",
    "    existing_data = pd.DataFrame(columns=['title', 'url', 'body', 'date'])\n",
    "    existing_urls = set()\n",
    "    logging.info(\"No existing file found. Creating a new dataset.\")\n",
    "\n",
    "# Scrape articles from page 1 to page 6\n",
    "all_articles = []\n",
    "for i in range(1, 8):  # Page numbers 1 to 6\n",
    "    page_url = f\"{base_url}/page/{i}\" if i > 1 else base_url\n",
    "    driver.get(page_url)\n",
    "    time.sleep(5)  # Wait for the page to load completely\n",
    "    all_articles.extend(get_article_urls_and_titles_newscomau(driver, visited_urls))\n",
    "\n",
    "# Now visit each URL and get the body content and date\n",
    "new_articles = []\n",
    "for article in all_articles:\n",
    "    if article['url'] not in existing_urls:\n",
    "        driver.get(article['url'])\n",
    "        time.sleep(2)  # Ensure the page has loaded\n",
    "        article_body, article_date = scrape_article_body_newscomau(driver)\n",
    "        article['body'] = article_body\n",
    "        article['date'] = article_date\n",
    "        new_articles.append(article)\n",
    "\n",
    "# Combine new articles with existing data\n",
    "if new_articles:\n",
    "    df_new_articles = pd.DataFrame(new_articles)\n",
    "    combined_data = pd.concat([existing_data, df_new_articles], ignore_index=True)\n",
    "    logging.info(\"New articles added.\")\n",
    "else:\n",
    "    combined_data = existing_data\n",
    "    logging.info(\"No new articles to add.\")\n",
    "\n",
    "# Save the combined data to a CSV file\n",
    "combined_data.to_csv(csv_file, index=False)\n",
    "logging.info(f\"Data saved to {csv_file}.\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "logging.info(\"Scraping completed.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}